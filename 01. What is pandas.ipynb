{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is pandas?\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Objectives\n",
    "\n",
    "+ Know why pandas is suitable for data analysis in Python\n",
    "+ Identify a DataFrame as \n",
    "\n",
    "### Resources\n",
    "\n",
    "+ [Official Documentation](http://pandas.pydata.org/pandas-docs/stable/)\n",
    "+ [Package Overview](http://pandas.pydata.org/pandas-docs/stable/overview.html)\n",
    "+ [Intro to Data Structures](http://pandas.pydata.org/pandas-docs/stable/dsintro.html)\n",
    "\n",
    "## Welcome to ....\n",
    "![][1]\n",
    "\n",
    "\n",
    "### What is pandas?\n",
    "pandas is one of the most popular open source data exploration libraries currently available. It gives its users the power to explore, manipulate, query, aggregate, and visualize **tabular** data. Tabular meaning data that is two-dimensional with rows and columns; i.e. a table.\n",
    "\n",
    "### Why pandas and not xyz?\n",
    "In this current age of data explosion, there are now many dozens of other tools that have many of the same capabilities as the pandas library. However, there are many aspects of pandas that make it an attractive choice for data analysis and it continues to have one of the fastest growing user bases.\n",
    "\n",
    "* It's a Python library and integrates well with the other popular data science libraries such as numpy, scikit-learn, statsmodels, matplotlib and seaborn.\n",
    "* It is nearly self-contained in that lots of functionality is built into one package. This contrasts with R, where many packages are needed to obtain the same functionality.\n",
    "* The community is excellent. Looking at Stack Overflow, for example, there are [many ten's of thousands of][2] pandas questions. If you need help, you are nearly guaranteed to find it very quickly. \n",
    "\n",
    "### Why is it named after an East Asian bear?\n",
    "\n",
    "The pandas library was begun by Wes McKinney beginning in 2008 at a hedge fund named AQR. Finance speak is to call tabular data 'panel data' which smashed together becomes pandas. If you are really interested in the history, you can hear it from the creator [himself][3].\n",
    "\n",
    "### Python already has data structures to handle data, why do we need another one?\n",
    "\n",
    "Even though Python is a high-level language, its primary built-in data structures lists and dictionaries, do not easily lend themselves to tabular data analysis in ways that humans can operate on them. \n",
    "\n",
    "### pandas is built directly on numpy\n",
    "\n",
    "[numpy][4] ('numerical Python') is the most popular third-party Python library for scientific computing and forms the foundation for dozens of others, including pandas. numpy's primary data structure is an n-dimensional array which is much more powerful than a Python list and with much better performance.\n",
    "\n",
    "All of the data in pandas is stored in numpy arrays. That said, it isn't necessary to know much about numpy when learning pandas. You can think of pandas as a higher-level, easier to use interface for doing data analysis than numpy. It is a good idea to eventually learn numpy, but for most tasks, pandas will be the right tool.\n",
    "\n",
    "### numpy tutorial in appendix\n",
    "\n",
    "Although it is not necessary to understand numpy to perform data analysis with pandas, it is a major piece of the data science ecosystem in Python and it can be used alongside pandas. A thorough numpy tutorial is available in Appendix A.\n",
    "\n",
    "## pandas operates on tabular (table) data\n",
    "\n",
    "There are numerous formats for data such as XML, JSON, raw bytes, and many others. But, for our purposes, we will only be examining what most people think of when they think of data - a table. pandas is built just for analyzing this tabular, rectangular, very deceptively normal concept of data. pandas has the capability to read in many different formats of data, but they all will be converted to tabular data.\n",
    "\n",
    "### The DataFrame and Series\n",
    "\n",
    "The DataFrame and Series are the two primary pandas objects that we will be using throughout this course.\n",
    "\n",
    "* **DataFrame** - A two-dimensional data structure that looks like any other rectangular table of data you have seen with rows and columns.\n",
    "* **Series** - A single dimension of data. It is analogous to a single column of data or a one dimensional array.\n",
    "\n",
    "[1]: images/pandas_logo.png\n",
    "[2]: http://stackoverflow.com/questions/tagged/pandas\n",
    "[3]: https://www.youtube.com/watch?v=kHdkFyGCxiY\n",
    "[4]: http://www.numpy.org/\n",
    "\n",
    "## pandas examples\n",
    "\n",
    "The rest of this chapter is dedicated to showing examples of what pandas is capable of doing. There will be one or two examples from each of the following major areas of the library.\n",
    "\n",
    "* Reading data\n",
    "* Filtering data\n",
    "* Aggregating methods\n",
    "* Non-Aggregating methods\n",
    "* Aggregating within groups\n",
    "* Tidying data\n",
    "* Joining data\n",
    "* Time series analysis\n",
    "* Visualization\n",
    "\n",
    "The goal is to give you a broad overview of what pandas is capable of doing. You are not expected to understand the syntax but rather get a few ideas of what you can expect to accomplish when using pandas. Explanations will be brief, but hopefully will provide just enough information so that you can logically follow what the end result is.\n",
    "\n",
    "### The `head` method\n",
    "\n",
    "You will notice that many of the last lines of code end with the `head` method. This returns, by default, the first five rows. This helps keep the output compact.\n",
    "\n",
    "## Reading data\n",
    "There will be multiple datasets used during the rest of this chapter. pandas can read in a variety of different data formats. The `read_csv` function is able to read in text data that is separated by a delimiter. By default, the delimiter is a comma. Below, we read in public bike usage data from the city of Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>usertype</th>\n",
       "      <th>gender</th>\n",
       "      <th>starttime</th>\n",
       "      <th>stoptime</th>\n",
       "      <th>tripduration</th>\n",
       "      <th>from_station_name</th>\n",
       "      <th>latitude_start</th>\n",
       "      <th>longitude_start</th>\n",
       "      <th>dpcapacity_start</th>\n",
       "      <th>to_station_name</th>\n",
       "      <th>latitude_end</th>\n",
       "      <th>longitude_end</th>\n",
       "      <th>dpcapacity_end</th>\n",
       "      <th>temperature</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7147</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-06-28 19:01:00</td>\n",
       "      <td>2013-06-28 19:17:00</td>\n",
       "      <td>993</td>\n",
       "      <td>Lake Shore Dr &amp; Monroe St</td>\n",
       "      <td>41.881050</td>\n",
       "      <td>-87.616970</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Michigan Ave &amp; Oak St</td>\n",
       "      <td>41.900960</td>\n",
       "      <td>-87.623777</td>\n",
       "      <td>15.0</td>\n",
       "      <td>73.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>mostlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7524</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-06-28 22:53:00</td>\n",
       "      <td>2013-06-28 23:03:00</td>\n",
       "      <td>623</td>\n",
       "      <td>Clinton St &amp; Washington Blvd</td>\n",
       "      <td>41.883380</td>\n",
       "      <td>-87.641170</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Wells St &amp; Walton St</td>\n",
       "      <td>41.899930</td>\n",
       "      <td>-87.634430</td>\n",
       "      <td>19.0</td>\n",
       "      <td>69.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>partlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10927</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-06-30 14:43:00</td>\n",
       "      <td>2013-06-30 15:01:00</td>\n",
       "      <td>1040</td>\n",
       "      <td>Sheffield Ave &amp; Kingsbury St</td>\n",
       "      <td>41.909592</td>\n",
       "      <td>-87.653497</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Dearborn St &amp; Monroe St</td>\n",
       "      <td>41.881320</td>\n",
       "      <td>-87.629521</td>\n",
       "      <td>23.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>mostlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12907</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-07-01 10:05:00</td>\n",
       "      <td>2013-07-01 10:16:00</td>\n",
       "      <td>667</td>\n",
       "      <td>Carpenter St &amp; Huron St</td>\n",
       "      <td>41.894556</td>\n",
       "      <td>-87.653449</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Clark St &amp; Randolph St</td>\n",
       "      <td>41.884576</td>\n",
       "      <td>-87.631890</td>\n",
       "      <td>31.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>mostlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13168</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-07-01 11:16:00</td>\n",
       "      <td>2013-07-01 11:18:00</td>\n",
       "      <td>130</td>\n",
       "      <td>Damen Ave &amp; Pierce Ave</td>\n",
       "      <td>41.909396</td>\n",
       "      <td>-87.677692</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Damen Ave &amp; Pierce Ave</td>\n",
       "      <td>41.909396</td>\n",
       "      <td>-87.677692</td>\n",
       "      <td>19.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>partlycloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_id    usertype gender            starttime             stoptime  \\\n",
       "0     7147  Subscriber   Male  2013-06-28 19:01:00  2013-06-28 19:17:00   \n",
       "1     7524  Subscriber   Male  2013-06-28 22:53:00  2013-06-28 23:03:00   \n",
       "2    10927  Subscriber   Male  2013-06-30 14:43:00  2013-06-30 15:01:00   \n",
       "3    12907  Subscriber   Male  2013-07-01 10:05:00  2013-07-01 10:16:00   \n",
       "4    13168  Subscriber   Male  2013-07-01 11:16:00  2013-07-01 11:18:00   \n",
       "\n",
       "   tripduration             from_station_name  latitude_start  \\\n",
       "0           993     Lake Shore Dr & Monroe St       41.881050   \n",
       "1           623  Clinton St & Washington Blvd       41.883380   \n",
       "2          1040  Sheffield Ave & Kingsbury St       41.909592   \n",
       "3           667       Carpenter St & Huron St       41.894556   \n",
       "4           130        Damen Ave & Pierce Ave       41.909396   \n",
       "\n",
       "   longitude_start  dpcapacity_start          to_station_name  latitude_end  \\\n",
       "0       -87.616970              11.0    Michigan Ave & Oak St     41.900960   \n",
       "1       -87.641170              31.0     Wells St & Walton St     41.899930   \n",
       "2       -87.653497              15.0  Dearborn St & Monroe St     41.881320   \n",
       "3       -87.653449              19.0   Clark St & Randolph St     41.884576   \n",
       "4       -87.677692              19.0   Damen Ave & Pierce Ave     41.909396   \n",
       "\n",
       "   longitude_end  dpcapacity_end  temperature  visibility  wind_speed  \\\n",
       "0     -87.623777            15.0         73.9        10.0        12.7   \n",
       "1     -87.634430            19.0         69.1        10.0         6.9   \n",
       "2     -87.629521            23.0         73.0        10.0        16.1   \n",
       "3     -87.631890            31.0         72.0        10.0        16.1   \n",
       "4     -87.677692            19.0         73.0        10.0        17.3   \n",
       "\n",
       "   precipitation        events  \n",
       "0        -9999.0  mostlycloudy  \n",
       "1        -9999.0  partlycloudy  \n",
       "2        -9999.0  mostlycloudy  \n",
       "3        -9999.0  mostlycloudy  \n",
       "4        -9999.0  partlycloudy  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bikes = pd.read_csv('../data/bikes.csv')\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas stores its data in a **DataFrame** which is the topic of the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering data\n",
    "pandas can filter the rows of a DataFrame based on whether the values in that row meet a condition. For instance, we can select only the rides that had a `tripduration` greater than 5000 (seconds). This example is a single condition that gets tested for each row. Only the rows that meet this condition are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>usertype</th>\n",
       "      <th>gender</th>\n",
       "      <th>starttime</th>\n",
       "      <th>stoptime</th>\n",
       "      <th>tripduration</th>\n",
       "      <th>from_station_name</th>\n",
       "      <th>latitude_start</th>\n",
       "      <th>longitude_start</th>\n",
       "      <th>dpcapacity_start</th>\n",
       "      <th>to_station_name</th>\n",
       "      <th>latitude_end</th>\n",
       "      <th>longitude_end</th>\n",
       "      <th>dpcapacity_end</th>\n",
       "      <th>temperature</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>40924</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-07-09 13:12:00</td>\n",
       "      <td>2013-07-09 14:42:00</td>\n",
       "      <td>5396</td>\n",
       "      <td>Canal St &amp; Jackson Blvd</td>\n",
       "      <td>41.878114</td>\n",
       "      <td>-87.639971</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Millennium Park</td>\n",
       "      <td>41.881032</td>\n",
       "      <td>-87.624084</td>\n",
       "      <td>35.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>61401</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Female</td>\n",
       "      <td>2013-07-14 14:08:00</td>\n",
       "      <td>2013-07-14 15:53:00</td>\n",
       "      <td>6274</td>\n",
       "      <td>Wabash Ave &amp; Roosevelt Rd</td>\n",
       "      <td>41.867173</td>\n",
       "      <td>-87.625955</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Lake Shore Dr &amp; Monroe St</td>\n",
       "      <td>41.881050</td>\n",
       "      <td>-87.616970</td>\n",
       "      <td>11.0</td>\n",
       "      <td>87.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>partlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>87005</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Female</td>\n",
       "      <td>2013-07-21 11:35:00</td>\n",
       "      <td>2013-07-21 13:54:00</td>\n",
       "      <td>8299</td>\n",
       "      <td>State St &amp; 19th St</td>\n",
       "      <td>41.856594</td>\n",
       "      <td>-87.627542</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Sheffield Ave &amp; Kingsbury St</td>\n",
       "      <td>41.909592</td>\n",
       "      <td>-87.653497</td>\n",
       "      <td>15.0</td>\n",
       "      <td>82.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>mostlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>323274</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-08-25 17:20:00</td>\n",
       "      <td>2013-08-25 19:26:00</td>\n",
       "      <td>7533</td>\n",
       "      <td>McClurg Ct &amp; Illinois St</td>\n",
       "      <td>41.891020</td>\n",
       "      <td>-87.617300</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Lake Shore Dr &amp; Monroe St</td>\n",
       "      <td>41.881050</td>\n",
       "      <td>-87.616970</td>\n",
       "      <td>11.0</td>\n",
       "      <td>87.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>442585</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-09-08 03:43:00</td>\n",
       "      <td>2013-09-08 07:20:00</td>\n",
       "      <td>13037</td>\n",
       "      <td>Dearborn Pkwy &amp; Delaware Pl</td>\n",
       "      <td>41.899007</td>\n",
       "      <td>-87.629928</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Sheridan Rd &amp; Irving Park Rd</td>\n",
       "      <td>41.954340</td>\n",
       "      <td>-87.654601</td>\n",
       "      <td>11.0</td>\n",
       "      <td>71.6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     trip_id    usertype  gender            starttime             stoptime  \\\n",
       "18     40924  Subscriber    Male  2013-07-09 13:12:00  2013-07-09 14:42:00   \n",
       "40     61401  Subscriber  Female  2013-07-14 14:08:00  2013-07-14 15:53:00   \n",
       "77     87005  Subscriber  Female  2013-07-21 11:35:00  2013-07-21 13:54:00   \n",
       "335   323274  Subscriber    Male  2013-08-25 17:20:00  2013-08-25 19:26:00   \n",
       "504   442585  Subscriber    Male  2013-09-08 03:43:00  2013-09-08 07:20:00   \n",
       "\n",
       "     tripduration            from_station_name  latitude_start  \\\n",
       "18           5396      Canal St & Jackson Blvd       41.878114   \n",
       "40           6274    Wabash Ave & Roosevelt Rd       41.867173   \n",
       "77           8299           State St & 19th St       41.856594   \n",
       "335          7533     McClurg Ct & Illinois St       41.891020   \n",
       "504         13037  Dearborn Pkwy & Delaware Pl       41.899007   \n",
       "\n",
       "     longitude_start  dpcapacity_start               to_station_name  \\\n",
       "18        -87.639971              35.0               Millennium Park   \n",
       "40        -87.625955              19.0     Lake Shore Dr & Monroe St   \n",
       "77        -87.627542              15.0  Sheffield Ave & Kingsbury St   \n",
       "335       -87.617300              23.0     Lake Shore Dr & Monroe St   \n",
       "504       -87.629928              19.0  Sheridan Rd & Irving Park Rd   \n",
       "\n",
       "     latitude_end  longitude_end  dpcapacity_end  temperature  visibility  \\\n",
       "18      41.881032     -87.624084            35.0         79.0        10.0   \n",
       "40      41.881050     -87.616970            11.0         87.1        10.0   \n",
       "77      41.909592     -87.653497            15.0         82.9        10.0   \n",
       "335     41.881050     -87.616970            11.0         87.1        10.0   \n",
       "504     41.954340     -87.654601            11.0         71.6        10.0   \n",
       "\n",
       "     wind_speed  precipitation        events  \n",
       "18         13.8            0.0        cloudy  \n",
       "40          8.1        -9999.0  partlycloudy  \n",
       "77          5.8        -9999.0  mostlycloudy  \n",
       "335        12.7        -9999.0         clear  \n",
       "504        11.5        -9999.0        cloudy  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt = bikes['tripduration'] > 5000\n",
    "bikes[filt].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Conditions\n",
    "\n",
    "We can test for multiple conditions in a single row. The following example only returns riders that are female **and** have a `tripduration` greater than 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt1 = bikes['tripduration'] > 5000\n",
    "filt2 = bikes['gender'] == 'Female'\n",
    "filt = filt1 & filt2\n",
    "bikes[filt].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example has multiple conditions but only requires that one of the conditions is true. It returns all the rows where either the rider is female or the `tripduration` is greater than 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = filt1 | filt2\n",
    "bikes[filt].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating methods\n",
    "\n",
    "The technical definition of an **aggregation** is when a sequence of values is summarized by a **single** number. For example sum, mean, median, min, and mix are all examples of aggregation functions. By default, calling these methods on a pandas DataFrame will apply the aggregation to each column. Below, we are using a dataset containing the percentage of undergraduate races for all US colleges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college = pd.read_csv('../data/college.csv', index_col='instnm')\n",
    "cr = college.loc[:, 'ugds_white':'ugds_unkn']\n",
    "cr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mean` method takes the mean of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas allows you to aggregate rows as well. You must use the `axis` parameter to change the direction of the aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-aggregating methods\n",
    "\n",
    "There are methods that perform some calculation on the DataFrame that do not aggregate the data. They preserve the shape of the DataFrame. For example, the `round` method will round each number to a particular decimal place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr.round(2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating within groups\n",
    "\n",
    "Above, we performed an aggregation on the entire DataFrame. We can instead, perform aggregations within groups of the data. Below we use an insurance dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = pd.read_csv('../data/insurance.csv')\n",
    "ins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest aggregations is count the frequency of occurrence of all the unique values within a single column. This is performed below with the `value_counts` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count frequency of one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins['region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wish to find the mean changes for each of the unique values found in the `sex` column. The `groupby` method gives us this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.groupby('sex').agg({'charges': 'mean'}).round(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple aggregation functions\n",
    "\n",
    "pandas allows us to perform multiple aggregations at the same time. Below, we find the mean, count the number of non-missing values, and the max of the `charges` column by each unique `sex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.groupby('sex').agg({'charges': ['mean', 'count', 'max']}).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Grouping columns\n",
    "\n",
    "pandas allows us form groups based on multiple columns. In the below example, each unique combination of `sex` and `region` for a group. There are 8 unique groups and for each of these groups the same aggregations as above are performed on the `charges` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.groupby(['sex', 'region']).agg({'charges': ['mean', 'count', 'max']}).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables\n",
    "We can reproduce the exact same output as above in a different shape with the `pivot_table` method. It groups and aggregates the same as `groupby` but places the unique values of one of the grouping columns as the new columns in the resulting DataFrame. Notice that pivot tables make for easier comparisons across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = ins.pivot_table(index='sex', columns='region', \n",
    "                     values='charges', aggfunc='mean').round(0)\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Styling DataFrames\n",
    "\n",
    "To help make your data really pop-out, pandas provides ways to style your DataFrame in various ways. Below, the maximum value of each column is highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.style.highlight_max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying\n",
    "\n",
    "Many datasets will be messy upon first inspection and pandas provides ways to clean them so that they are put into 'tidy' form.\n",
    "\n",
    "### Options in the `read_csv` function\n",
    "\n",
    "Below, we read in a new dataset on plane crashes. Notice all the question marks. They represent missing values, but by default pandas will read them in as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pd.read_csv('../data/tidy/planecrashinfo.csv')\n",
    "pc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_csv` has dozens of options to help read in messy data. Of the options allows you to convert a particular string to missing values. Notice that all of the question marks are now `NaN` (not a number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pd.read_csv('../data/tidy/planecrashinfo.csv', na_values='?')\n",
    "pc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String manipulation\n",
    "\n",
    "Often times there is data stuck within a string column that you will need to extract. The `aboard` column appears to have three distinct pieces of information; the total number of people on board, the number of passengers, and the number of crew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aboard = pc['aboard']\n",
    "aboard.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas has special functionality for manipulating strings. Below, we use a regular expression to extract the pertinent numbers from the `aboard` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aboard.str.extract(r'(\\d+)?\\D*(\\d+)?\\D*(\\d+)?').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping into tidy form\n",
    "\n",
    "Occasionally, you will have several columns of data that all belong in a single column. Take a look at the DataFrame below on average arrival delay of airlines at different airports. All the columns with three-letter airport codes could be placed in the same column as they all contain the arrival delay which has the same units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aad = pd.read_csv('../data/tidy/average_arrival_delay.csv').head()\n",
    "aad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `melt` method stacks columns one on top of the other. Here, it places all of the three-letter airport code columns into a single column. The first two airports (ATL and DEN) are shown below in the new tidy DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aad.melt(id_vars='airline', var_name='airport', value_name='delay').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Data\n",
    "pandas can join multiple DataFrames together by matching values in one or more columns. If you are familiar with SQL, then pandas performs joins in a similar fashion. Below, we make a connection to a database and read in two of its tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///../data/neurIPS.db')\n",
    "\n",
    "authors = pd.read_sql('Authors', engine)\n",
    "pa = pd.read_sql('PaperAuthors', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the first 5 rows of each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now join these tables together using the `merge` method. The `AuthorID` column from the `pa` table is aligned with the `Id` column of the `authors` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.merge(authors, how='left', left_on='AuthorId', right_on='Id').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "\n",
    "One of the original purposes of pandas was to do time series analysis. Below, we read in the last 5 years of Apple's stock price data with help from the excellent [IEX Trading API][0].\n",
    "\n",
    "[0]: https://iextrading.com/developer/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = pd.read_json('https://api.iextrading.com/1.0/stock/aapl/chart/5y')\n",
    "aapl = aapl.set_index('date')\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a period of time\n",
    "\n",
    "pandas allows us to easily select a period of time. Below, we select all of the trading data from February 20, 2018 through March 5, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl['2018-02-20':'2018-03-05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by time\n",
    "\n",
    "We can group by some length of time. Here, we group together every month of trading data and return the average closing price of that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_mc = aapl.resample('M').agg({'close':'mean'})\n",
    "aapl_mc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "pandas provides basic visualization abilities by giving its users a few nice default plots. Below, we plot the average monthly closing price of Apple for the last 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "aapl_mc.plot(kind='line', figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the college race data to create a box plot of each of the race percentage columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr.plot(kind='box', figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn our pivot table from above into a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.plot(kind='bar', figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much More\n",
    "\n",
    "The above was just a small sampling that pandas has to offer, but does show many basic examples from many of the major sections of the library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
